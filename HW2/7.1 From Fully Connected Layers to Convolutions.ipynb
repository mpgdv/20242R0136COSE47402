{"cells":[{"cell_type":"markdown","metadata":{"id":"TrODypBJQb6v"},"source":["# COSE474-2024: Deep Learning HW2"]},{"cell_type":"markdown","source":["# 7.1 From Fully Connected Layers to Convolutions"],"metadata":{"id":"YVEXfr1vWVYh"}},{"cell_type":"markdown","source":["## Discussions"],"metadata":{"id":"6xjLvKb_qYLm"}},{"cell_type":"markdown","source":["### 7.1.1 Invariance"],"metadata":{"id":"Hx8bFBQnb4-b"}},{"cell_type":"markdown","source":["- Convolutional neural networks (CNNs) systematize the idea of spatial invariance, exploiting it to learn useful representations with fewer parameters\n","  - Spatial invariance: if an object occurs in any image it will be detected irrespective of its position\n","\n","Enumerating a few desiderata to guide our design of a neural network architecture suitable for computer vision:\n","1. **Translation invariance** (or translation equivariance): In the earliest layers, our network should respond similarly to the same patch, regardless of where it appears in the image.\n","2. **Locality principle**: The earliest layers of the network should focus on local regions, without regard for the contents of the image in distant regions.\n","3. As we proceed, deeper layers should be able to capture longer-range features of the image, in a way similar to higher level vision in nature"],"metadata":{"id":"JukioxD_Z7AC"}},{"cell_type":"markdown","source":["### 7.1.2 Constraining the MLP"],"metadata":{"id":"uXPCJCU6ckE8"}},{"cell_type":"markdown","source":["We can consider an MLP with two-dimensional images **X** asn inputs and their immediate hidden representations **H** similarly represented as matrices.\n","Fourth-order weight tensors **W**. **U** contains biases.\n","\n","Fully connected layer:\n","$$\n","[\\mathbf{H}]_{i,j} = [\\mathbf{U}]_{i,j} + \\sum_{a} \\sum_{b} [\\mathbf{V}]_{i,j,a,b}[\\mathbf{X}]_{i+a,j+b}\n","$$\n","\n","$[\\mathbf{V}]_{i,j,a,b} = [\\mathbf{W}]_{i,j,i+a,j+b}$  The indices and run over both positive and negative offsets, covering the entire image. For this parametrization: a 1000 x 1000 image is mapped to a 1000 x 1000 hidden representation. This requires  $10^{12}$ parameters.\n","\n","**Translation Invariance**\n","\n","This implies that a shift in the input **X** should simply lead to a shift in the hidden representation **H**. This is only possible if **V** and **U** do not actually depend on (i,j).\n","\n","We can simplify the definition for **H**:\n","$$\n","[\\mathbf{H}]_{i,j} = u + \\sum_{a} \\sum_{b} [\\mathbf{V}]_{a,b}[\\mathbf{X}]_{i+a,j+b}$$ This is a convolution. **V** no longer depends on the location withi. The number of parameters is now 4 x $10^6$\n","\n","**Locality**\n","We believe that we should not have to look very far away from location (i,j) in order to glean relevant information to assess what is going on at $[H]_{i,j}$ Outside some range $|a| > \\Delta$ or $|b| > \\Delta$, we should set $[V]_{a,b} = 0$\n","\n","$$\n","[\\mathbf{H}]_{i,j} = u + \\sum_{a= -\\Delta}^{\\Delta} \\sum_{b= - \\Delta}^{\\Delta} [\\mathbf{V}]_{a,b}[\\mathbf{X}]_{i+a,j+b}$$\n","The number of parameters are now 4$\\Delta^2$, where $\\Delta$ is typically smaller than 10.\n","- Convolutional neural networks are a special family of neural networks that contain convolutional layers.\n","- **V** is refered to as a *convolution kernel*, a *filter*, or simply the layer's weights that are learnable parameters."],"metadata":{"id":"mXt4Xk1qcmrh"}},{"cell_type":"markdown","source":["### 7.1.3 Convolutions"],"metadata":{"id":"qumJRBWflRhr"}},{"cell_type":"markdown","source":["In mathematics, the *convolution* between two functions, say $f, g : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is defined as\n","$$(f*g)(\\text{x}) = \\int f(\\text{z})g(\\text{x - z})d\\text{z}$$\n","Whenever we have discrete objects, the integral turns into a sum.\n","- For vectors from the set of square-summable infinite-dimensional vectors with index running over $\\mathbb{Z}$: $(f*g)(i) = \\sum_a f(a)g(i-a)$\n","- For two-dimensional tensors, we have a corresponding sum with indices (a,b) for $f$ and $(i-a,j-b)$ for $g$, respectively: $(f*g)(i,j) = \\sum_a \\sum_b f(a,b)g(i-a,j-b)$"],"metadata":{"id":"LEHwx9vVlVEH"}},{"cell_type":"markdown","source":["### 7.1.4 Channels"],"metadata":{"id":"GW3M0eHlnfdR"}},{"cell_type":"markdown","source":["- The convolutional layer picks windows of a given size and weighs intensities according to the filter **V**\n","- Images are third-order tensors, characterized by height, width, and channel. The channel can be regarded as assigning a multidimensional representation to each pixel location.\n","  - We index **X** as [**X**]$_{i,j,k}$. The convolutional filter has to adapt accordingly. [**V**]$_{a,b,c}$\n","- We want an entire vector of hidden representations corresponding to each spatial location. These are sometimes called *channels*, *feature maps*.\n","- To support multiple channels in both inputs (X) and hidden representations (H), we can add a fourth coordinate to V.\n","$$\n","[\\mathbf{H}]_{i,j,d} = u + \\sum_{a= -\\Delta}^{\\Delta} \\sum_{b= - \\Delta}^{\\Delta}\\sum_c [\\mathbf{V}]_{a,b,c,d}[\\mathbf{X}]_{i+a,j+b,c}$$ where $d$ indexes the output channels in the hidden representations H."],"metadata":{"id":"Y_VsGPEgnoI9"}},{"cell_type":"markdown","source":["- Translation invariance in images implies that all patches of an image will be treated in the same manner.\n","- Locality means that only a small neighborhood of pixels will be used to compute the corresponding hidden representations.\n","- Some of the earliest references to CNNs are in the form of the Neocognitron\n","- Adding channels allowed us to bring back some of the complexity that was lost due to the restrictions imposed on the convolutional kernel by locality and translation invariance."],"metadata":{"id":"G1EUm7HCpmtI"}}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPjNyl5tPNJaL7YtcfEeUCV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"markdown","source":["# 5.3 Forward Propagation, Backward Propagation, and Computational Graphs"],"metadata":{"id":"YVEXfr1vWVYh"}},{"cell_type":"markdown","source":["## Discussions & Exercises"],"metadata":{"id":"6L1HdauwW9ks"}},{"cell_type":"markdown","source":["#### 5.3.1 Forward Propagation"],"metadata":{"id":"QV9RNV3jKZcL"}},{"cell_type":"markdown","source":["* Forward propagation (or forward pass) refers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input layer to the output layer\n","* Input example: $ \\mathbf{x} \\in \\mathbb{R}^d $\n","* Intermediete variable:  $\\mathbf{ z} = \\mathbf{W}^{(1)} \\mathbf{x}$\n","* Hidden activation vector of length h: $\\mathbf{h} = \\phi(\\mathbf{z})$\n","  * The hidden layer output h is also an intermediate variable\n","* Output layer variable with a vector of length q: $\\mathbf{ o} = \\mathbf{W}^{(2)} \\mathbf{h}$\n","* Loss term of a single data example. $l$ is the loss function, and $y$ the example label: $ L = l(\\mathbf{o},y) $\n","* $l_{2}$ regularization, hyperparameter $\\lambda$. Regularization term:\n","$$COPIARLO AL RATO$$\n","* Model's regularized loss on a fiven data example:\n","$$J = L + s$$"],"metadata":{"id":"4eOYVJrdKfaW"}},{"cell_type":"markdown","source":["#### 5.3.2 Computational Graph of Forward Propagation"],"metadata":{"id":"-o8RwWazSpec"}},{"cell_type":"markdown","source":["* Plotting computational graphs helps us visualize the dependencies of operators and variables within the calculation."],"metadata":{"id":"S24PDEczSwFt"}},{"cell_type":"markdown","source":["#### 5.3.3 Backpropagation"],"metadata":{"id":"pr3NNoKwS36w"}},{"cell_type":"markdown","source":["* Backpropagation refers to the method of calculating the gradient of neural network parameters.\n","* The method traverses the network in reverse order, from the output to the input layer, according to the chain rule from calculus. The algorithm stores any intermediate variables (partial derivatives) required while calculating the gradient with respect to some parameters.\n","* Assume that we have functions Y = f(X) and Z = g(Y), in which the input and the output X, Y, Z are tensors of arbitrary shape. By using the chain rule, we can compute the derivative of Z with respect to X via:\n","  $$\\frac{\\partial \\text{Z}}{\\partial \\text{X}} = \\text{prod }(\\frac{\\partial \\text{Z}}{\\partial \\text{Y}},\n","  \\frac{\\partial \\text{Y}}{\\partial \\text{X}})$$\n","* The parameters of the simple network with one hidden layer are $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$.\n","* The objective of backpropagation is to calculate the gradients $\\partial \\text{J}$ / $\\partial \\mathbf{W}^{(1)}$ and $\\partial \\text{J}$ / $\\partial \\mathbf{W}^{(2)}$\n","  *  To accomplish this, we apply the chain rule and calculate, in turn, the gradient of each intermediate variable and parameter. The order of calculations are reversed relative to those performed in forward propagation, since we need to start with the outcome of the computational graph and work our way towards the parameters."],"metadata":{"id":"og0yfqCrS8iN"}},{"cell_type":"markdown","source":["#### 5.3.4 Training Neural Networks"],"metadata":{"id":"FrivfoRFW8E1"}},{"cell_type":"markdown","source":["* When training neural networks, forward and backward propagation depend on each other.\n","  * for forward propagation, we traverse the computational graph in the direction of dependencies and compute all the variables on its path. These are then used for backpropagation where the compute order on the graph is reversed.\n","* When training neural networks, once model parameters are initialized, we alternate forward propagation with backpropagation, updating model parameters using gradients given by backpropagation.\n","* Training deeper networks using larger batch sizes more easily leads to out-of-memory errors."],"metadata":{"id":"ISLEdPM2W_Q8"}},{"cell_type":"markdown","source":["### Exercises"],"metadata":{"id":"QSroNYD7cNrL"}},{"cell_type":"markdown","source":["1. Assume that the inputs  X to some scalar function  f are  nxm matrices. What is the dimensionality of the gradient of  f with respect to  X?\n","\n","Also n x m"],"metadata":{"id":"1dvA6c1HX3kW"}}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7nU7wjR4K2necoB8Q+nDX"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}